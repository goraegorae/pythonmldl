{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM을 사용한 영화 리뷰 감정 분석\n",
    "\n",
    "영화 리뷰와 같은 데이터는 일반적인 수치 데이터와 달리 사람의 언어, 즉 자연어 데이터입니다. 자연어 데이터는 머신러닝에서 특히 다루기 까다로운 데이터인데요. 데이터 자체가 길고 전처리할 부분이 많을 뿐만 아니라 순서나 단어의 유사성 개념을 이해해야만 하기 때문입니다.\n",
    "\n",
    "딥러닝에서는 이렇듯 순서가 있는 데이터를 처리하기 위해 RNN (Recurrent Neural Network)를 사용하고, 일반적인 RNN에서 발생하는 문제를 막기 위한 다양한 종류의 개량된 레이어를 사용합니다.\n",
    "\n",
    "단어의 유사성을 학습하는 것은, 단어를 단순히 1, 2, 53과 같은 숫자로 표현하는 대신 [0.3, 0.4, 2.1] 과 같은 좌표로 표현하는 것을 통해 가능합니다. 좌표 공간에서의 방향이나 거리를 인간이 이해하는 단어 사이의 관계를 표현하기 위해 쓰는 것이죠. 이를 word embedding이라고 하며 자연어 처리 분야에서 거의 필수적인 요소입니다.\n",
    "\n",
    "LSTM(RNN) 소개  \n",
    "https://brunch.co.kr/@chris-song/9\n",
    "\n",
    "Word Embedding (Word2Vec)  \n",
    "https://deeplearning4j.org/kr/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras의 백엔드 프레임워크로 Tensorflow를 사용합니다\n",
    "import tensorflow as tf\n",
    "\n",
    "# 실습을 진행하기 위해 선생님 그래픽카드 중 어떤 카드를 쓸지\n",
    "# 해당 카드의 GPU 메모리를 몇 % 사용할지 설정하는 부분입니다\n",
    "# 실습을 위한 것이므로 일반적인 환경에서는 필요 없습니다 (몰라도 됨)\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# 이 셀을 실행하고 *이 사라진 것을 확인 후 다음으로 진행하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 3000\n",
    "index_from_num = 3\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=index_from_num)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 300\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "X_alt_train = X_train[..., numpy.newaxis]\n",
    "X_alt_test = X_test[..., numpy.newaxis]\n",
    "X = X_train\n",
    "X_t = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To see original text data\n",
    "(X_original_train, y_original_train), (X_original_test, y_original_test) = imdb.load_data(index_from=index_from_num)\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+index_from_num) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_only():\n",
    "    global X\n",
    "    global X_t\n",
    "    X = X_train\n",
    "    X_t = X_test\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(max_review_length, )))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple():\n",
    "    global X\n",
    "    global X_t\n",
    "    X = X_alt_train\n",
    "    X_t = X_alt_test\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(max_review_length, 1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_cuDNN():\n",
    "    global X\n",
    "    global X_t\n",
    "    X = X_alt_train\n",
    "    X_t = X_alt_test\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(128, input_shape=(max_review_length, 1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stacked():\n",
    "    global X\n",
    "    global X_t\n",
    "    X = X_alt_train\n",
    "    X_t = X_alt_test\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(128, input_shape=(max_review_length, 1), return_sequences=True))\n",
    "    model.add(CuDNNLSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_vector_length = 32\n",
    "def embedding():\n",
    "    global X\n",
    "    global X_t\n",
    "    X = X_train\n",
    "    X_t = X_test\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "    model.add(CuDNNLSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_vector_length = 32\n",
    "def embedding_stacked():\n",
    "    global X\n",
    "    global X_t\n",
    "    X = X_train\n",
    "    X_t = X_test\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "    model.add(CuDNNLSTM(128, return_sequences=False))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dense_only()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_t, y_test, batch_size=32, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "end_index = 10\n",
    "predict = model.predict(X_t[start_index:end_index+1])\n",
    "\n",
    "for i, p in enumerate(predict):\n",
    "    if (p[0] < 0.5 and y_test[i] == 0) or (p[0] > 0.5 and y_test[i] == 1):\n",
    "        correct = '맞음'\n",
    "    else:\n",
    "        correct = '틀림'\n",
    "    print(\"%d번 데이터 (%s) - 예측: %.3f / 실제: %d\" % (start_index + i, correct, p[0], y_test[i]))\n",
    "    print(' '.join(id_to_word[id] for id in X_original_test[i] if id != 0 and id != 1 ))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
